La target est class, 7 features.

data_plot.png, correlations.png : grosses corrélations entre 
- Area et ConvexArea
- Area et Perimeter
- MajorAxisLength et Perimeter
- ConvexArea et Perimeter

equilibre_des_classes.png : pas de classe inbalance, bonne nouvelle.




########
###---c- représentation graphique
########
library(FactoMineR)

# on fait la visu "à la main"
res.pca2 = PCA(cbind(seeds,grp=as.factor(variety)), 
               quali.sup = 8, graph=FALSE)
plot(res.pca2,habillage=8)
plot(res.pca2,axes=2:3,habillage=8)

# visu des classifications de 2 à 5 groupes
 
# on sauvegarde l'environnement avant le changement de marge
old.par <- par(no.readonly = TRUE)
# pour profiter au maximum de l'espace disponible
par (mfrow=c(2,2), mai=c(0.7,0.7,0.4,0.2))

for (i in 2:5) {
  grp = cutree(res1,h=rev(res1$height)[i])
  plot(res.pca2$ind$coord[,1],res.pca2$ind$coord[,2],
       col=grp, pch=grp,
       main=paste("nb de groupes=",i),
       xlim=c(-4,5),ylim=c(-4,4),cex=0.5,xlab="Dim 1",ylab="Dim 2")
}

# puis on utilise fviz_Cluster
library(factoextra)
# elle demande d'utiliser hcut qui appelle hclust + cutree
hc.cut = hcut(seeds, k = 4, hc_method = "complete")
sum(hc.cut$cluster!=cutree(res1,k=4)) #petite vérification

# les 4 graphes. On remarque que l'axe 1 est en miroir par
# rapport à notre programmation: 
# suivant le type de méthode utiliser pour calculer les directions
# propres u ou -u sont choisis
# cela ne change pas l'interprétation

plt =lapply(2:5, function(k) fviz_cluster(hcut(seeds, k = k, hc_method = "complete"),
                                          main="complete"))
cowplot::plot_grid(plt[[1]],plt[[2]],plt[[3]],plt[[4]],nrow=2,ncol=2)

# dendrogram avec groupes:

fviz_dend(hc.cut, show_labels = FALSE, rect = TRUE)

########
###---d- autre distance
########
res2 = hclust(dist(seeds),method="ward.D2")

pltw =lapply(2:4, function(k) fviz_cluster(hcut(seeds, k = k, hc_method = "ward.D2"),
                                           main="ward.D2"))
cowplot::plot_grid(plt[[1]],plt[[2]],plt[[3]],
                   pltw[[1]],pltw[[2]],pltw[[3]],nrow=2,ncol=3)

# plot alluvial
library(alluvial)
alluvial(data.frame(complete=cutree(res1,k=4),
                    wardD2=cutree(res2,k=4)), 
         freq = rep(1,n), #col=2, 
         col = c("red", "blue", "green","orange")[cutree(res1,k=4)])


#comparaison numérique des partition avec l'index de Rand
mclust::adjustedRandIndex(cutree(res2,k=4), cutree(res1,k=4))


########
###---  centrer réduire?
########
seedscr = data.frame(scale(seeds))
rescr = hclust(dist(seedscr),method="ward.D2")
par(mfrow=c(1,2))
barplot(rev(rescr$height)[1:15],main="diagramme des hauteurs")
abline(h=15) 

# l'éboulis  définit trois groupes
plot(rescr,cex=0.5) # dendrogramme
abline(h=15)         # 3 classes

mclust::adjustedRandIndex(cutree(res2,k=3), cutree(rescr,k=3)) #0.68
par(mfrow=c(1,1))
alluvial(data.frame(brut=cutree(res2,k=3),
                    reduit=cutree(rescr,k=3)), 
         freq = rep(1,n), #col=2, 
         col = c("red", "blue", "green","orange")[cutree(res2,k=3)])

# La standardisation évite que les variables à grande échelle domine pour la contruction des clusters. 
mclust::adjustedRandIndex(cutree(rescr,k=3),variety)#0.79
mclust::adjustedRandIndex(cutree(res2,k=3),variety) #0.71

########
###---e- Clustering de variables
########
# on choisit une dissemblance adaptée aux variables
dd = as.dist((1 - cor(seeds))/2)
par(mfrow=c(1,1))
plot(hclust(dd,method="ward.D2")) 































## Clustering par Classification hiérarchique ascendante


###---a- cah: méthode "complete" par défaut avec distance euclidienne
res1 <- hclust(dist(data[, -p]))
dendrogramme <- ggdendrogram(res1, rotate = TRUE)
print(dendrogramme)

#cutree(res1, h = 6) # classification à une hauteur donnée
#éboulis des hauteurs
#barplot(rev(res1$height)[1:15], main = "diagramme des hauteurs")
#abline(h = 6)
# indice de silhouette
#library(cluster)
#sil = silhouette(cutree(res1,k=4), dist(data[, -p]))
#mean(sil[,3])         # indice moyen 0.37
#fviz_silhouette(sil)  # visualisation




